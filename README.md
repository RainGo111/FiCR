<div align="center">


  <h1>ðŸ”¥ FiCR Ontology Portal</h1>

  <p align="center">
    <b>Fire Compliance and Risk Analysis (FiCR) Ontology</b>
    <br />
    <i>A semantic web platform for fire safety engineering and building compliance</i>
  </p>

  <br />

  <!-- Soft Style Live Demo Badge -->
  <a href="https://RainGo111.github.io/FiCR/">
    <img src="https://img.shields.io/badge/ðŸš€_Live_Demo-Click_to_Open-3b82f6?style=flat-square&logoColor=white" alt="Live Demo" />
  </a>

  <br />
  <br />
</div>

---

## ðŸ“– About The Project

The **FiCR Ontology Portal** is a specialized web application designed to bridge the gap between complex semantic models and practical fire safety engineering. It serves as a comprehensive interface for the **Fire Compliance and Risk Analysis (FiCR)** ontology, enabling researchers, engineers, and domain experts to explore, understand, and implement fire safety standards in the Semantic Web.

This platform provides:
- An interactive **Ontology Browser** over the raw `.ttl` ontology files
- A **SPARQL Query Lab** for live querying against a GraphDB knowledge graph
- A **FiCR Chatbot** powered by LLM that takes building survey JSON and produces fire compliance reports through an automated pipeline
- A **Fire Risk Report** page with compliance analysis results

---

## âœ¨ Core Modules

### ðŸ” Ontology Browser (Documentation)
An interactive, searchable index of all classes and properties defined in the FiCR namespace.
*   **Class Hierarchy**: View parent-child relationships.
*   **Property Definitions**: Detailed domain and range specifications.
*   **Smart Search**: Instantly find terms without browsing the entire tree.

### ðŸ§ª SPARQL Query Lab
Direct programmatic access to the FiCR Knowledge Graph via GraphDB.
*   **Preset Queries**: Curated SPARQL queries organized by module (Inventory, Compliance, Risk).
*   **Custom Editor**: Write and execute your own SPARQL queries.
*   **Live Results**: Interactive results table with URI shortening.

### ðŸ¤– FiCR Chatbot
An LLM-powered fire compliance analysis pipeline with a chat interface.
*   **Upload** a building survey JSON file (e.g., `duplex_a_survey.json`).
*   **Automated Pipeline**: Validate JSON â†’ Build RDF knowledge graph â†’ Run 14 SPARQL compliance queries â†’ Generate LLM report.
*   **Streaming Output**: Report is streamed in real-time from the LLM.
*   **Download**: Export the generated report as a `.md` file.
*   **Multi-LLM Support**: Choose from Claude, OpenAI, Gemini, DeepSeek, or Zhipu GLM.

### ðŸ“Š Fire Risk Report
A pre-computed compliance report page driven by live GraphDB data.
*   **KPI Metrics**: Color-coded compliance rates.
*   **Print-to-PDF**: Generate printable reports.

### ðŸ—ºï¸ Roadmap
Product roadmap with capability preview.

---

## ðŸ› ï¸ Getting Started

### Prerequisites
*   **Node.js** (v16+) and **npm** â€” for the frontend
*   **Python** (3.10+) and **pip** â€” for the chatbot backend
*   At least **one LLM API key** (Anthropic, OpenAI, Google, DeepSeek, or Zhipu GLM) â€” for the chatbot

### Installation

1.  **Clone the repository**
    ```bash
    git clone https://github.com/RainGo111/FiCR.git
    cd FiCR
    ```

2.  **Install frontend dependencies**
    ```bash
    npm install
    ```

3.  **Install backend dependencies**
    ```bash
    cd backend
    pip install -r requirements.txt
    ```

4.  **Configure LLM API keys**
    ```bash
    # In the backend/ directory:
    cp .env.example .env
    # Edit .env and add at least one API key
    ```

### Running Locally

You need **two terminals** to run the full application:

**Terminal 1 â€” Backend (Python FastAPI)**
```bash
cd backend
uvicorn server:app --port 8000 --reload
```

**Terminal 2 â€” Frontend (Vite)**
```bash
npm run dev
```

Then open **http://localhost:5173** in your browser.

### Using the Chatbot

1.  Navigate to the **FiCR Chatbot** page.
2.  Select an LLM provider and model from the toolbar dropdown.
3.  Load a sample survey (e.g., "Duplex A") or paste/upload your own survey JSON.
4.  Click **Analyze** â€” the pipeline will:
    - Validate the survey JSON against the `ficr-survey-v1` schema
    - Convert it to an RDF knowledge graph (ABox)
    - Run 14 SPARQL compliance queries against the merged TBox + regulatory config
    - Stream a fire compliance report from the selected LLM
5.  Once complete, click **Download Report (.md)** to save.

> **Note**: The chatbot requires the Python backend to be running locally. The static pages (Documentation, Roadmap) work without the backend. The Query Lab and Report pages require a running GraphDB instance.

---

## ðŸ“¦ Project Structure

```
â”œâ”€â”€ public/                          # Static ontology files
â”‚   â”œâ”€â”€ ficr_tbox_0.13.0.ttl        # FiCR ontology (TBox)
â”‚   â””â”€â”€ ficr_demo_0.13.0.ttl        # Demo instance data
â”œâ”€â”€ src/                             # React frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ chatbot/                 # Chat UI components
â”‚   â”‚   â”œâ”€â”€ documentation/           # Ontology browser components
â”‚   â”‚   â”œâ”€â”€ layout/                  # Header, Footer, Layout
â”‚   â”‚   â””â”€â”€ shared/                  # Card, Button, CodeBlock, etc.
â”‚   â”œâ”€â”€ pages/                       # Route pages
â”‚   â”‚   â”œâ”€â”€ Home.tsx
â”‚   â”‚   â”œâ”€â”€ Documentation.tsx
â”‚   â”‚   â”œâ”€â”€ QueryLab.tsx
â”‚   â”‚   â”œâ”€â”€ Chatbot.tsx              # LLM chatbot page
â”‚   â”‚   â”œâ”€â”€ Report.tsx
â”‚   â”‚   â””â”€â”€ Roadmap.tsx
â”‚   â”œâ”€â”€ content/                     # Site config, preset queries
â”‚   â”œâ”€â”€ hooks/                       # Custom React hooks
â”‚   â””â”€â”€ utils/                       # TTL/RDF parsers
â”œâ”€â”€ backend/                         # Python pipeline backend
â”‚   â”œâ”€â”€ server.py                    # FastAPI server (SSE streaming)
â”‚   â”œâ”€â”€ pipeline.py                  # 4-stage pipeline orchestrator
â”‚   â”œâ”€â”€ ficr_json_to_rdf.py         # Stage 2: JSON â†’ RDF converter
â”‚   â”œâ”€â”€ ficr_sparql_runner.py        # Stage 3: SPARQL query executor
â”‚   â”œâ”€â”€ prompts/                     # LLM system prompts
â”‚   â”œâ”€â”€ schemas/                     # JSON Schema (ficr-survey-v1)
â”‚   â”œâ”€â”€ references/                  # TBox, regulatory config, SPARQL queries, sample data
â”‚   â”œâ”€â”€ tests/                       # Schema & SPARQL tests
â”‚   â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚   â””â”€â”€ .env.example                 # API key template
â”œâ”€â”€ supabase/                        # Supabase edge functions
â”œâ”€â”€ .github/workflows/deploy.yml     # GitHub Pages deployment
â”œâ”€â”€ package.json                     # Node.js dependencies
â”œâ”€â”€ vite.config.ts                   # Vite config with API proxies
â””â”€â”€ tailwind.config.js               # Tailwind CSS theme
```

---

## ðŸ”§ Configuration

### Frontend Environment (`.env` in project root)
```env
GRAPHDB_URL=http://localhost:7200/repositories/FiCR
GRAPHDB_USER=admin
GRAPHDB_PASS=root
CHATBOT_API_URL=http://localhost:8000
```

### Backend Environment (`backend/.env`)
Copy from `backend/.env.example` and add your API keys:
```env
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=AI...
DEEPSEEK_API_KEY=sk-...
GLM_API_KEY=...
```

You only need **one** provider configured to use the chatbot.

---

## ðŸ“„ License & Acknowledgments

**FiCR Ontology Authors**:
*   Maxime LefranÃ§ois
*   Pieter Pauwels
*   Georg Ferdinand Schneider
*   Mads Holten Rasmussen

*Portal implementation is licensed under MIT. Ontology content retains its original license.*
